{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelling Change - Project\n",
    "\n",
    "## Topic: Gradient Descent\n",
    "\n",
    "> `numerical` `optimisation`\n",
    "\n",
    "Gradient descent is a general method for finding minima of functions taught is widely used in many fields.\n",
    "Write out the equations for a simple gradient descent method and code an implementation in Python.\n",
    "Locate the minima of an example function who's min you can find using the analytically (that is, as we did\n",
    "in lectures and tutorials). Investigate how the convergence is affected by:\n",
    "\n",
    "1. step size or other parameters in the algorith\n",
    "2. the initial starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report details the mathematics and computer code implementation of the _Gradient Descent_ optimisation method, and investigates the behaviour of the optimisation process when subject to variance in algorithm paramaters ('step size', iteration number) and variance in the initial starting point. \n",
    "\n",
    "It is found that TODO TODO TODO\n",
    "\n",
    "### Report Structure\n",
    "\n",
    "1. [**Introduction to the Pure-Python3 Implementation.**](#1.-Introduction-to-the-Pure-Python3-Implementation)\n",
    "2. [**Introduction to the Gradient Descent method.**](#2.-Introduction-to-the-Gradient-Descent-Method)\n",
    "3. Blah\n",
    "4. Blah\n",
    "5. [**References**](#References)\n",
    "6. [**Appendix**](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to the Pure-Python3 Implementation\n",
    "\n",
    "To assist in the exploration and communication of the Gradient Descent optimisation technique, a 'from scratch' [pure-Python](https://stackoverflow.com/a/52461357/4885590) implementation of the gradient descent algorithm has been written and is used throughout. It is a 'standalone module', and imports no third-party code. The implementation targets usefulness in learning, not performance, but is fast enough for practical example.\n",
    "\n",
    "To reduce implementation complexity and length, not all differentiable functions are supported by the implementation. Supported functions include:\n",
    "\n",
    "- [Polynomial functions](https://en.wikipedia.org/wiki/Polynomial)\n",
    "- A limited set of trigonometric functions, to explore nonlinear functions\n",
    "\n",
    "\n",
    "The implementation is wholly contained in one Python3 module, `gradient_descent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "import gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Introduction to the Gradient Descent Method\n",
    "\n",
    "Gradient Descent is an iterative optimization process that can be used effectively to find local mimina of differentiable functions, particulary when those functions are convex. When the output of a differentiable function under some set of inputs can be framed as a _cost_, the minimization of this _cost function_ becomes an optimization problem to which the Gradient Descent process can be applied.\n",
    "\n",
    "The \"Deep Neural Networks\" revolution that swept through the 2010s has its foundation in the simple single-layer neural networks first published in the 1980s, and those simple networks were optimized through gradient descent. Thus, a first lesson in understanding today's hottest technological field, Deep Neural Networks, involves going right back to the start and understanding the basic Gradient Descent optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 A Function's Gradient\n",
    "\n",
    "In order to minimise a function's value, we need to ascertain which way we should nudge its inputs to decrease the output value, and we have to be sure than a series of decreases will eventually lead to a minimum (local or global). For differentiable functions, the first-derivative of a function can be the way.\n",
    "\n",
    "For a function of a single variable, $f(x)$, the rate of change at some value $x=a$ is given by the first-derivative $f'(x)$. In the case of $f(x) = x^2 + x$, we know that:\n",
    "\n",
    "$$f'(x) = 2x + 1$$\n",
    "\n",
    "and thus at $f'(1) = 2(1) + 1 = 3$ the function is increasing in output value 'to the right' and decreasing 'to the left'. At $f'(-1) = 2(-1) + 1 = -1$ the function is decreasing in output value 'to the right' and increasing 'to the left;. In either case, we know from the first-derivative which direction to nudge $x$, until we reach $f'(1/2) = 2*(1/2) + 1 = 0$ and we've reached the critical point.\n",
    "\n",
    "\n",
    "But for a multi-variable function there are multiple ways in which to influence the output value and thus multiple dimensions along which a we could change inputs. How can we extend our understanding of the direction of function decrease beyond 'left and right' and into 3-dimensions and more? We use partial derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $z = f(x, y)$, then we have a multi-variable function with partial derivatives:\n",
    "    \n",
    "$$f_x(x_0, y_0) = \\lim_{h \\to 0}\\frac{f(x_0+h, y_0) - f(x_0, y_0)}{h}$$\n",
    "\n",
    "$$f_y(x_0, y_0) = \\lim_{h \\to 0}\\frac{f(x_0, y_0+h) - f(x_0, y_0)}{h}$$\n",
    "\n",
    "with each capturing the rate-of-change with respect to a single variable in our multi-variable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the $x,y$ plane, we can imagine being at some point $(x_0,y_0)$ and nudging away from that point in the plane by the vector $\\mathbf{u} = \\langle a, b \\rangle $. \n",
    "\n",
    "Now not restricted to moving 'left and right' in the x-axis or 'up and down' the y-axis, we have a **Directional Derivative** of $f$ at $(x_0,y_0)$.\n",
    "\n",
    "$$D_uf(x_0, y_0) = \\lim_{h \\to 0}\\frac{f(x_0 + ha, y_0+hb) - f(x_0, y_0)}{h}$$\n",
    "\n",
    "More intuitively, we can consider that nudge as being of length $h$ at some angle $\\theta$ (capturing direction). Thus our $a$ and $b$ are $\\cos{\\theta}$ and $\\sin{\\theta}$ respectively, and $\\mathbf{u} = \\langle a, b \\rangle $ is a vector of length 1.\n",
    "\n",
    "In fact, any... TODO\n",
    "\n",
    "\n",
    "To prove this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function $g$ of the single variable $h$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(h) & = f(x_0 + ha, y_0+hb) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "By definition of the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "g'(0) & = \\lim_{h \\to 0}\\frac{g(h) - g(0)}{h} \\\\\n",
    "& = \\lim_{h \\to 0}\\frac{f(x_0+ha, y_0+hb) - f(x_0, y_0)}{h}\\\\\n",
    "& = D_uf(x_0, y_0)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Writing $x = x_0 + ha$ and $y = y_0 + hb$ we get $g(h) = f(x, y)$ and \n",
    "\n",
    "\\begin{align}\n",
    "g'(h) & = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial h} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial h} \\\\\n",
    "& = f_x(x, y)a + f_y(x, y)b\n",
    "& = D_uf(x_0, y_0)\n",
    "\\end{align}\n",
    "\n",
    "g(h) = f(x_0 + ha, y_0+hb) \\\\\n",
    "\n",
    "\\text{then by}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the `gradient_descent` library, we can calculate the gradient vector from a `MultiVariableFunction`. For the function:**\n",
    "\n",
    "$$f(x,y) = x^2 + y^2 - 2x - 6y + 14$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\u001b[3my\u001b[23m: 2\u001b[3my\u001b[23m¹ + -6, \u001b[3mx\u001b[23m: 2\u001b[3mx\u001b[23m¹ + -2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = gradient_descent.Variable(\"x\")\n",
    "y = gradient_descent.Variable(\"y\")\n",
    "f = gradient_descent.MultiVariableFunction(\n",
    "    variables={x, y},\n",
    "    expressions=[\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=1, exponent=2),\n",
    "        gradient_descent.PolynomialExpression(variable=y, coefficient=1, exponent=2),\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=-2, exponent=1),\n",
    "        gradient_descent.PolynomialExpression(variable=y, coefficient=-6, exponent=1),\n",
    "        gradient_descent.ConstantExpression(real=14.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "f.gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Steepest Descent\n",
    "\n",
    "Now able to determine the gradient vector of a function, capturing the rate of change along each dimension of a function, the question becomes in which 'direction' to go to 'descend' or decrease the function's value?.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gradient Descent - Iterating Towards the Bottom\n",
    "\n",
    "Now with a method to calculate the direction of maximum descent from a point $\\mathbf{a}$ in a function's input space, we are very close to creating the _Gradient Descent_ optimisation process.\n",
    "\n",
    "Given a differentiable multi-variable function $f(\\mathbf{x})$, with $\\mathbf{x}$ being a vector of inputs $\\langle x, y, z, ... \\rangle$, then we know:\n",
    "\n",
    "**At some point $\\mathbf{a} \\in \\mathbf{x}$, $f(\\mathbf{x})$ decreases _fastest_ in the direction of the negative gradient:** $-\\nabla \\mathbf{f(a)}$\n",
    "\n",
    "In the Python library `gradient_descent`, we can calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: {\u001b[3my\u001b[23m: 2\u001b[3my\u001b[23m¹ + -6, \u001b[3mx\u001b[23m: 2\u001b[3mx\u001b[23m¹ + -2}\n",
      "Gradient of f(x, y) @ point 'a'\n",
      "{\u001b[3my\u001b[23m: -4, \u001b[3mx\u001b[23m: -4}\n"
     ]
    }
   ],
   "source": [
    "f_grad = f.gradient()\n",
    "\n",
    "print(f\"Gradient: {f_grad}\")\n",
    "\n",
    "a: gradient_descent.Point = {\n",
    "    x: -1,\n",
    "    y: 1,\n",
    "}\n",
    "\n",
    "f_grad_a: Mapping[gradient_descent.Variable, float] = {\n",
    "    var: grad_elem.evaluate(a)\n",
    "    for var, grad_elem\n",
    "    in f_grad.items()\n",
    "}\n",
    "    \n",
    "print(\"Gradient of f(x, y) @ point 'a'\")\n",
    "print(f_grad_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  Analytical vs. Iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, understanding the process, we are ready to run Gradient Descent in Python. The optimisation problem we'll solve is minimising:\n",
    "\n",
    "$$cost= f(x,y) = x^2 + y^2 - 2x - 6y + 14$$\n",
    "\n",
    "We can solve this analytically, which will be useful in validating the Python implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "f_x(x, y) & = 2x + 0 - 2 - 0 + 0 \\\\\n",
    "f_x(x, y) & = 2x - 2\\\\\n",
    "\\\\\n",
    "f_y(x, y) & = 0 + 2y - 0 - 6 + 0 \\\\\n",
    "f_y(x, y) & = 2y - 6\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Solving...\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_x(x, y) & = 2x - 2 = 0 \\\\\n",
    "2x - 2 & = 0 \\\\\n",
    "2x & = 2 \\\\\n",
    "x & = 1 \\\\\n",
    "\\\\\n",
    "f_y(x, y) & = 2y - 6 = 0 \\\\\n",
    "2y - 6 & = 0 \\\\\n",
    "2y & = 6 \\\\\n",
    "y & = 3 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So $f(x, y)$ has a critical point at $(1, 3)$ and we can show graphically that this critical point is a minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2494f9b4f745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's solve the same problem using the Python implementation of the Gradient Descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0. Current min estimate: {\u001b[3my\u001b[23m: 2.2, \u001b[3mx\u001b[23m: 2.6}\n",
      "Iteration 10. Current min estimate: {\u001b[3my\u001b[23m: 2.91410065408, \u001b[3mx\u001b[23m: 1.1717986918399999}\n",
      "Iteration 20. Current min estimate: {\u001b[3my\u001b[23m: 2.9907766279631454, \u001b[3mx\u001b[23m: 1.0184467440737097}\n",
      "Iteration 30. Current min estimate: {\u001b[3my\u001b[23m: 2.9990096479685717, \u001b[3mx\u001b[23m: 1.0019807040628568}\n",
      "Iteration 40. Current min estimate: {\u001b[3my\u001b[23m: 2.999893661760337, \u001b[3mx\u001b[23m: 1.0002126764793258}\n",
      "Iteration 50. Current min estimate: {\u001b[3my\u001b[23m: 2.9999885820184584, \u001b[3mx\u001b[23m: 1.0000228359630834}\n",
      "Iteration 60. Current min estimate: {\u001b[3my\u001b[23m: 2.9999987740035676, \u001b[3mx\u001b[23m: 1.0000024519928654}\n",
      "Iteration 70. Current min estimate: {\u001b[3my\u001b[23m: 2.999999868359635, \u001b[3mx\u001b[23m: 1.0000002632807292}\n",
      "Iteration 80. Current min estimate: {\u001b[3my\u001b[23m: 2.9999999858652235, \u001b[3mx\u001b[23m: 1.000000028269553}\n",
      "Iteration 90. Current min estimate: {\u001b[3my\u001b[23m: 2.99999999848229, \u001b[3mx\u001b[23m: 1.0000000030354201}\n",
      "Iteration 100. Current min estimate: {\u001b[3my\u001b[23m: 2.9999999998370375, \u001b[3mx\u001b[23m: 1.0000000003259257}\n",
      "Iteration 110. Current min estimate: {\u001b[3my\u001b[23m: 2.999999999982502, \u001b[3mx\u001b[23m: 1.000000000034996}\n",
      "Iteration 120. Current min estimate: {\u001b[3my\u001b[23m: 2.9999999999981215, \u001b[3mx\u001b[23m: 1.0000000000037577}\n",
      "Iteration 130. Current min estimate: {\u001b[3my\u001b[23m: 2.9999999999997984, \u001b[3mx\u001b[23m: 1.0000000000004035}\n",
      "Iteration 140. Current min estimate: {\u001b[3my\u001b[23m: 2.9999999999999782, \u001b[3mx\u001b[23m: 1.0000000000000433}\n",
      "Iteration 150. Current min estimate: {\u001b[3my\u001b[23m: 2.999999999999998, \u001b[3mx\u001b[23m: 1.0000000000000047}\n",
      "Iteration 160. Current min estimate: {\u001b[3my\u001b[23m: 2.999999999999999, \u001b[3mx\u001b[23m: 1.0000000000000004}\n",
      "Iteration as not changed value. Stopping early.\n",
      "\n",
      "Results:\n",
      "Min Value: 4.0000000000000036\n",
      "Min Location: {\u001b[3my\u001b[23m: 2.999999999999999, \u001b[3mx\u001b[23m: 1.0000000000000004}\n"
     ]
    }
   ],
   "source": [
    "minimum_val, minimum_point = gradient_descent.gradient_descent(\n",
    "        gamma=0.1,\n",
    "        max_iterations=5000,\n",
    "        f=f,\n",
    "    )\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Min Value: {minimum_val}\")\n",
    "print(f\"Min Location: {minimum_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The answers are not exact because of [floating-point arithmetic error](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html), but $(1, 3)$ is correct.\n",
    "\n",
    "We can re-run the function and no matter which values are randomly assigned to the initial starting point, the process converges to the correct result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convergence Behaviour\n",
    "\n",
    "Having demonstrated gradient descent convergence of a simple convex function, let's investigate convergence behaviour\n",
    "on different functions when the parameters of the convergence process are manipulated.\n",
    "\n",
    "#### 3.1 Changing Max Iterations\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Changing Step Size\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Contours of f(x,y)')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "delta = 0.001\n",
    "x = np.arange(-3.0, 3.0, delta)\n",
    "y = np.arange(-2.0, 2.0, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z1 = np.exp(-X**2 - Y**2)\n",
    "Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n",
    "Z = (Z1 - Z2) * 2\n",
    "\n",
    "x = np.linspace(-2,2)\n",
    "y = np.linspace(-2,2)\n",
    "\n",
    "# Create our grid of points\n",
    "xv, yv = np.meshgrid(x,y)\n",
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "x_extrema = np.array([-1,1])\n",
    "y_extrema = np.array([-1,1])\n",
    "z_extrema = (x_extrema**2)+(y_extrema**2) \n",
    "\n",
    "# Make a contour plot that is filled with color.\n",
    "ax.contourf(xv,yv, (1 - xv)**2+ 100*(yv - (xv**2))**2)\n",
    "ax.scatter(x_extrema, y_extrema, z_extrema,  color='r')\n",
    "ax.set_title('Contours of f(x,y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x,y)=(1-x)^2+100(y-x^2)^2$$\n",
    "\n",
    "alternate form\n",
    "\n",
    "$$100 x^4 - 200 x^2 y + x^2 - 2 x + 100 y^2 + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Changing initial starting points\n",
    "\n",
    "TODO - function with two local minima and one global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*  RUMELHART, David E.; HINTON, Geoffrey E.; WILLIAMS, Ronald J. (1986). \"Learning representations by back-propagating errors\". Nature. 323 (6088): 533–536. doi:10.1038/323533a0. S2CID 205001834 -http://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf\n",
    "* STEWART, J. (2019). \"Calculus: concepts and contexts\". Boston, MA, USA, Cengage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "The full `gradient_descent` implementation is available online at [github.com/thundergolfer/modelling_change/](https://github.com/thundergolfer/modelling_change/), but it has also been copied in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO copy implementation of `gradient_descent`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
